{
 "metadata": {
  "name": "",
  "signature": "sha256:7100a9686e657b9e0e0dee4b1e9edd68bcf3e8531bde2a11b212adbc9cabaab6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import math\n",
      "import random\n",
      "import collections\n",
      "from itertools import compress\n",
      "\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "from gensim.models import Word2Vec\n",
      "from gensim.models.word2vec import LineSentence\n",
      "\n",
      "file_path='/home/qorwns/textmining/data/160911_test2/Output/keywords.txt'\n",
      "\n",
      "def read_keywords(file_path):\n",
      "    with open(file_path, 'r') as f:\n",
      "        data = f.read().split()\n",
      "    return data\n",
      "\n",
      "\n",
      "words = read_keywords(file_path)\n",
      "vocabulary_size = len(words)\n",
      "print('key data size', len(words))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "key data size 58655\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = []\n",
      "count.extend(collections.Counter(words).most_common(len(words)))\n",
      "print('Most Common Keywords')\n",
      "print(count[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most Common Keywords\n",
        "[('adsorption', 272), ('mofs', 228), ('material', 195), ('framework', 194), ('co2', 170), ('compound', 168), ('metalorganic', 168), ('ligand', 161), ('metal', 161), ('porous', 146)]\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_dataset(words):\n",
      "    count = []\n",
      "    count.extend(collections.Counter(words).most_common(len(words)))\n",
      "    dictionary = dict()\n",
      "    for word, _ in count:\n",
      "        dictionary[word] = len(dictionary)\n",
      "    data = list()\n",
      "    \n",
      "    for word in words:\n",
      "        if word in dictionary:\n",
      "            index = dictionary[word]\n",
      "        data.append(index)\n",
      "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
      "    return data, count, dictionary, reverse_dictionary\n",
      "\n",
      "\n",
      "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
      "del words # hint to reduce memory\n",
      "print (data[:20])\n",
      "a = [reverse_dictionary[i] for i in data[:20]]\n",
      "print(a)\n",
      "print('keyword set',len(dictionary))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[4863, 4164, 12669, 203, 307, 16, 56, 7697, 3562, 2486, 3442, 207, 3180, 168, 3981, 2921, 3242, 1926, 2210, 2008]\n",
        "['curcumin', 'cdmofs', 'cyclodextrinmetal', 'cd', 'ph', 'complex', 'interaction', 'emittance', 'cdmof', 'benign', 'absorbance', 'enhance', 'dissolve', 'formation', 'loaded', 'food', 'dissociation', 'potassium', 'phenolic', 'cyclodextrin']\n",
        "keyword set 14892\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_index = 0\n",
      "'''\n",
      "def generate_batch(batch_size, num_skips, skip_window):\n",
      "    global data_index\n",
      "    assert batch_size % num_skips == 0\n",
      "    assert num_skips <= skip_window\n",
      "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
      "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
      "    \n",
      "    span = skip_window\n",
      "    buff = collections.deque(maxlen=span)\n",
      "    for _ in range(span):\n",
      "        buff.append(data[data_index])\n",
      "        data_index = (data_index + 1) % len(data)\n",
      "\n",
      "    for i in range(batch_size // num_skips):\n",
      "        target = skip_window\n",
      "        target_to_avoid = [skip_window]\n",
      "        for j in range(num_skips):\n",
      "            while target in target_to_avoid:\n",
      "                target = random.randint(0, span -1)\n",
      "            target_to_avoid.append(target)\n",
      "            batch[i * num_skips + j] = buff[skip_window]\n",
      "            labels[i * num_skips + j,0] = buff[target]\n",
      "\n",
      "        buff.append(data[data_index])\n",
      "        data_index = (data_index + 1) % len(data)\n",
      "\n",
      "    return batch, labels\n",
      "    \n",
      "'''\n",
      "def generate_batch(batch_size, num_skips, skip_window):\n",
      "    '''\n",
      "        data : list of index\n",
      "        batch_size :  num of mini-batch\n",
      "        num_skips : num of pairs in a context window\n",
      "        skip_window : context window size\n",
      "    '''\n",
      "    global data_index\n",
      "    assert batch_size % num_skips == 0\n",
      "    assert num_skips <= 2 * skip_window\n",
      "\n",
      "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
      "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
      "\n",
      "    span = 2 * skip_window + 1\n",
      "    buff = collections.deque(maxlen=span)\n",
      "\n",
      "    for _ in range(span):\n",
      "        buff.append(data[data_index])\n",
      "        data_index = (data_index + 1) % len(data)\n",
      "\n",
      "    for i in range(batch_size // num_skips):\n",
      "        target = skip_window\n",
      "        target_to_avoid = [skip_window]\n",
      "        for j in range(num_skips):\n",
      "            while target in target_to_avoid:\n",
      "                target = random.randint(0, span -1)\n",
      "            target_to_avoid.append(target)\n",
      "            batch[i * num_skips + j] = buff[skip_window]\n",
      "            labels[i * num_skips + j,0] = buff[target]\n",
      "        #mask[skip_window] = 0\n",
      "        #batch[i,:] = list(compress(buff, mask))\n",
      "        #labels[i,0] = buff[skip_window]\n",
      "        buff.append(data[data_index])\n",
      "        data_index = (data_index + 1) % len(data)\n",
      "\n",
      "    return batch, labels\n",
      "    \n",
      "    \n",
      "batch, labels = generate_batch(batch_size=400, num_skips=10, skip_window=9)\n",
      "\n",
      "\n",
      "print('Generate Batch')\n",
      "for i in range(20):\n",
      "    print(batch[i], reverse_dictionary[batch[i]],'->',reverse_dictionary[labels[i,0]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Generate Batch\n",
        "2486 benign -> complex\n",
        "2486 benign -> loaded\n",
        "2486 benign -> cdmofs\n",
        "2486 benign -> dissociation\n",
        "2486 benign -> curcumin\n",
        "2486 benign -> cyclodextrinmetal\n",
        "2486 benign -> emittance\n",
        "2486 benign -> formation\n",
        "2486 benign -> dissolve\n",
        "2486 benign -> cd\n",
        "3442 absorbance -> phenolic\n",
        "3442 absorbance -> complex\n",
        "3442 absorbance -> loaded\n",
        "3442 absorbance -> enhance\n",
        "3442 absorbance -> dissolve\n",
        "3442 absorbance -> cd\n",
        "3442 absorbance -> formation\n",
        "3442 absorbance -> cdmof\n",
        "3442 absorbance -> benign\n",
        "3442 absorbance -> cyclodextrin\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "batch_size = 100\n",
      "embedding_size = 100\n",
      "skip_window = 9\n",
      "num_skips = 10\n",
      "\n",
      "valid_size = 16\n",
      "valid_window = 100\n",
      "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
      "num_sampled = 64\n",
      "\n",
      "graph = tf.Graph()\n",
      "\n",
      "with graph.as_default():\n",
      "\n",
      "    # Input data.\n",
      "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
      "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
      "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
      "\n",
      "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
      "    # embedding_lookup\uc774 GPU implementation\uc774 \uad6c\ud604\uc774 \uc548\ub418\uc5b4 \uc788\uc5b4\uc11c CPU\ub85c \ud574\uc57c\ud568.\n",
      "    # default\uac00 GPU\ub77c\uc11c \uba85\uc2dc\uc801\uc73c\ub85c CPU\ub77c\uace0 \uc9c0\uc815\ud574\uc90c.\n",
      "    with tf.device('/cpu:0'):\n",
      "        # Look up embeddings for inputs.\n",
      "        # embedding matrix (vectors)\n",
      "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
      "        # \uc804\uccb4 embedding matrix\uc5d0\uc11c train_inputs (mini-batch; indices) \uc774 \uac00\ub9ac\ud0a4\ub294 \uc784\ubca0\ub529 \ubca1\ud130\ub9cc\uc744 \ucd94\ucd9c\n",
      "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
      "\n",
      "        # Construct the variables for the NCE loss\n",
      "        # NCE loss \ub294 logistic regression model \uc744 \uc0ac\uc6a9\ud574\uc11c \uc815\uc758\ub41c\ub2e4.\n",
      "        # \uc989, logistic regression \uc744 \uc704\ud574, vocabulary\uc758 \uac01 \ub2e8\uc5b4\ub4e4\uc5d0 \ub300\ud574 weight\uc640 bias\uac00 \ud544\uc694\ud568.\n",
      "        nce_weights = tf.Variable(\n",
      "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
      "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
      "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
      "\n",
      "    # Compute the average NCE loss for the batch.\n",
      "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
      "    # time we evaluate the loss.\n",
      "    loss = tf.reduce_mean(\n",
      "        tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
      "                       num_sampled, vocabulary_size))\n",
      "\n",
      "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
      "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
      "\n",
      "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
      "    # minibatch (valid_embeddings) \uc640 all embeddings \uc0ac\uc774\uc758 cosine similarity\ub97c \uacc4\uc0b0\ud55c\ub2e4.\n",
      "    # \uc774 \uacfc\uc815\uc740 \ud559\uc2b5\uc774 \uc9c4\ud589\ub418\uba74\uc11c \uac01 valid_example \ub4e4\uc5d0\uac8c \uac00\uc7a5 \uac00\uae4c\uc6b4 \ub2e8\uc5b4\uac00 \uc5b4\ub5a4 \uac83\uc778\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud568\uc774\ub2e4 (\uc989 \ud559\uc2b5 \uacfc\uc815\uc744 \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud568).\n",
      "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
      "    normalized_embeddings = embeddings / norm\n",
      "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
      "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 100001\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "    # We must initialize all variables before we use them.\n",
      "    tf.initialize_all_variables().run()\n",
      "    print(\"Initialized\")\n",
      "\n",
      "    average_loss = 0\n",
      "    for step in range(num_steps):\n",
      "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
      "        feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
      "\n",
      "        # We perform one update step by evaluating the optimizer op (including it\n",
      "        # in the list of returned values for session.run()\n",
      "        # feed_dict\ub97c \uc0ac\uc6a9\ud574\uc11c placeholder\uc5d0 \ub370\uc774\ud130\ub97c \uc9d1\uc5b4\ub123\uace0 \ud559\uc2b5\uc2dc\ud0b4.\n",
      "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
      "        average_loss += loss_val\n",
      "\n",
      "        if step % 2000 == 0:\n",
      "            if step > 0:\n",
      "                average_loss = average_loss / 2000\n",
      "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
      "            print(\"Average loss at step \", step, \": \", average_loss)\n",
      "            average_loss = 0\n",
      "\n",
      "\n",
      "    final_embeddings = normalized_embeddings.eval()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialized\n",
        "Average loss at step  0 :  284.01675415\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000 :  147.38961558\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4000 :  77.4013339548\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6000 :  46.8941071737\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8000 :  30.3700025246\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10000 :  22.6832611718\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12000 :  16.3587473214\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14000 :  12.3411413691\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 16000 :  10.7710359101\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 18000 :  9.1964522171\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20000 :  8.42872024012\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 22000 :  7.44239453983\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24000 :  7.09200545931\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 26000 :  6.51247246695\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 28000 :  6.2049975493\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 30000 :  6.06348584437\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 32000 :  5.85277395082\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 34000 :  5.76847392368\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 36000 :  5.56168112493\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 38000 :  5.47777288032\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 40000 :  5.4465705663\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 42000 :  5.30165297854\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 44000 :  5.31349486017\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 46000 :  5.20379157376\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 48000 :  5.1674544251\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50000 :  5.12240912592\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 52000 :  5.0789215368\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 54000 :  5.08588221061\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 56000 :  5.03243214953\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 58000 :  5.01777148461\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 60000 :  4.94152380288\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 62000 :  4.87574525094\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 64000 :  4.89117972112\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 66000 :  4.84946127558\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 68000 :  4.86685593069\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 70000 :  4.81828735089\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 72000 :  4.77211151183\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 74000 :  4.75079856145\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 76000 :  4.7049752723\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 78000 :  4.72517642021\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 80000 :  4.68681531751\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 82000 :  4.66507362378\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 84000 :  4.62701979923\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 86000 :  4.57240027177\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 88000 :  4.59171470761\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 90000 :  4.55375938618\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 92000 :  4.54264162254\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 94000 :  4.50697232723\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 96000 :  4.46325745177\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 98000 :  4.45655270243\n",
        "Average loss at step "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100000 :  4.42838698637\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
      "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
      "    plt.figure(figsize=(18, 18))  #in inches\n",
      "    for i, label in enumerate(labels):\n",
      "        x, y = low_dim_embs[i,:]\n",
      "        plt.scatter(x, y)\n",
      "        plt.annotate(label,\n",
      "                     xy=(x, y),\n",
      "                     xytext=(5, 2),\n",
      "                     textcoords='offset points',\n",
      "                     ha='right',\n",
      "                     va='bottom')\n",
      "\n",
      "    plt.savefig(filename)\n",
      "\n",
      "try:\n",
      "    # \ud639\uc2dc \uc5ec\uae30\uc11c \uc5d0\ub7ec\uac00 \ub09c\ub2e4\uba74, scikit-learn \uacfc matplotlib \uc744 \ucd5c\uc2e0\ubc84\uc804\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8\ud558\uc790.\n",
      "    from sklearn.manifold import TSNE\n",
      "    import matplotlib.pyplot as plt\n",
      "\n",
      "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
      "    plot_only = 10000\n",
      "    #plot_only = 100\n",
      "\n",
      "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n",
      "    labels = [reverse_dictionary[i] for i in range(plot_only)]\n",
      "    plot_with_labels(low_dim_embs, labels[:200])\n",
      "    #plot_with_labels(low_dim_embs, labels)\n",
      "\n",
      "except ImportError:\n",
      "    print(\"Please install sklearn and matplotlib to visualize embeddings.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary['tcnq'], dictionary['conductivity'], dictionary['screening'], dictionary['isotherm'], dictionary['iast'], dictionary['separation']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "(2576, 163, 617, 189, 2379, 10)"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "low_dim_embs[2576], low_dim_embs[163], low_dim_embs[617], low_dim_embs[189], low_dim_embs[2379], low_dim_embs[10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "(array([ 2.37480509, -0.67365445]),\n",
        " array([ 1.62998997,  0.57557668]),\n",
        " array([-0.87935862, -0.40952259]),\n",
        " array([-0.10452055, -0.83070198]),\n",
        " array([-1.70046074, -0.84648929]),\n",
        " array([-0.96640043, -1.51465399]))"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_cos_sim(idx1, idx2):\n",
      "    cos_sim = np.dot(final_embeddings[idx1],final_embeddings[idx2])\n",
      "    return cos_sim\n",
      "\n",
      "print('tcnq & conductivity', get_cos_sim(2576,163))\n",
      "print('screening & isotherm', get_cos_sim(617,189))\n",
      "print('iast & separation', get_cos_sim(2379,10))\n",
      "print('screening & conductivity', get_cos_sim(617,163))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tcnq & conductivity 0.397601\n",
        "screening & isotherm 0.648602\n",
        "iast & separation 0.608216\n",
        "screening & conductivity 0.407755\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_most_similar(word, length=10):\n",
      "    idx = dictionary[word]\n",
      "    coor_self = final_embeddings[idx]\n",
      "    sim_list = [np.dot(coor_self, coor) for coor in final_embeddings] \n",
      "    idx_list = range(len(final_embeddings))\n",
      "    sim_list, idx_list = (list(t) for t in zip(*sorted(zip(sim_list, idx_list), reverse=True)))\n",
      "    idx_list = idx_list[1:length+1]\n",
      "    word_list = [reverse_dictionary[i] for i in idx_list]\n",
      "    print (word_list)\n",
      "\n",
      "get_most_similar('iast',10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['separation', 'scn', 'kedge', 'mixture', 'selectivity', 'selfpenetration', 'great', 'flexibleachiral', 'binaphthalenederived', 'nonsinglecrystal']\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_most_similar('tcnq',10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['agpf', 'substituents', 'brick', 'theory', 'plasmonic', 'structure', 'isostructural', 'sulfobifunctionalized', 'pyridylligand', 'solventmodified']\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_most_similar('screening')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['simulation', 'adsorption', 'isotherm', 'mixedmatrix', 'computational', 'screen', 'theory', 'material', 'cuso4', 'molecularlevel']\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_most_similar('dft')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['uio66based', 'temperature', 'ifp6', 'area', 'metalorganic', 'tetracarboxylates', 'adsorption', 'phosphoric', 'symmetryadapted', 'ndimethylformamide']\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_most_similar('interpenetration')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['interpenetrated', 'binaphthalenederived', 'ligand', 'metalorganic', 'structure', 'crystalline', 'tetradentate', 'framework', 'noninterpenetrated', 'unprecedented']\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_most_similar('selectivity')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['separation', 'adsorption', 'gas', 'preferentially', 'adsorbent', 'n2', 'co2', 'effect', 'mixture', 'theory']\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_most_similar('metalorganic')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['framework', 'property', 'mofs', 'highly', 'demonstrate', 'porous', 'type', 'ligand', 'construct', 'structure']\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_most_similar('gcmc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['cuso4', 'chlorogenic', 'nott206a', 'endergonic', 'method', 'exploit', 'frameworkh', 'metalsubstitution', 'praseodymium', 'experimental']\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}