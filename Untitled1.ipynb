{
 "metadata": {
  "name": "",
  "signature": "sha256:8c142f549d6e8e3d0e876ba356ab3767625baa66bb280451b9a8aa700800d73f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pubmed import *\n",
      "\n",
      "PubMed = PubMedTokenizer(0, 'pubmed_result.txt','pubmed_result.xml', 1,1,1)\n",
      "\n",
      "man_keyword_list = PubMed.line_to_structed_data()[6]\n",
      "\n",
      "print(man_keyword_list[0])\n",
      "\n",
      "man_index_list=[]\n",
      "for i, key in enumerate(man_keyword_list):\n",
      "    if key:\n",
      "        man_index_list.append(i)\n",
      "print(len(man_index_list))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PubMed used\n",
        "PubMed starting index = 0\n",
        "['cd-mofs', 'curcumin', 'encapsulation', 'spectroscopy', 'stability', '\u03b3-cyclodextrin']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "895\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Textmining import *\n",
      "import os\n",
      "\n",
      "CORPUS_OPTION = (0,0,0,1,0,0,0,1)\n",
      "BASIC_PATH = ('','','','','','','')\n",
      "main_path = os.getcwd()\n",
      "\n",
      "Corpus = Make_custom_corpus(['a','b','c'],CORPUS_OPTION,BASIC_PATH,0)\n",
      "total_corpus = Corpus.build_corpus(main_path+'/data'+'/160911_test/tokens/',5000)\n",
      "\n",
      "print(len(total_corpus))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2977\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TFIDF_OPTION = (1,0,1.0,1,(1,1),1)\n",
      "TFIDF_model = TFIDF_Vectorizer_scikit(total_corpus, TFIDF_OPTION,BASIC_PATH)\n",
      "TFIDF_model.run()\n",
      "com_keyword_list = TFIDF_model.list_for_keywordset(10)\n",
      "print(com_keyword_list[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TFIDF matrix shape is\n",
        "(2977, 15563)\n",
        "The number of TFIDF unique word set is\n",
        "15563\n",
        "{1920, 6786, 2533, 4397, 1101, 1937, 10579, 1942, 3158, 3223}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stop_words = get_stopword_set()\n",
      "\n",
      "sents= []\n",
      "for i in man_index_list:\n",
      "    sent = ''\n",
      "    for word in man_keyword_list[i]:\n",
      "        sent = sent+word+' '\n",
      "    sents.append(sent)\n",
      "    \n",
      "    \n",
      "total_sents=[]\n",
      "for j in sents:\n",
      "    tokens = get_tokens(j)\n",
      "    pos_token = nltk.pos_tag(tokens)\n",
      "    filtered_tokens = [word for word in pos_token if not word[0] in stop_words]\n",
      "    lemma_first = lemmatize_tokens_for_pos(filtered_tokens, lemmatizer)\n",
      "    lemmatized_tokens = [word for word in lemma_first if not word in stop_words]\n",
      "    remove_num = [word for word in lemmatized_tokens if word>'a'  and word.isalnum() and not word[0].isdigit()]\n",
      "    final_word = []\n",
      "    for word in remove_num:\n",
      "        if len(word) == 2:\n",
      "            if not is_useful_2gram(word):\n",
      "                continue\n",
      "            else:\n",
      "                final_word.append(word)\n",
      "        else:\n",
      "            final_word.append(word)\n",
      "            \n",
      "    total_sents.append(final_word)\n",
      "\n",
      "print(len(total_sents))\n",
      "man_keyword_list = total_sents\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "895\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TFIDF_total_word = TFIDF_model.get_unique_word()\n",
      "\n",
      "new_key=[]\n",
      "for i in man_index_list:\n",
      "    a = [TFIDF_total_word[index] for index in list(com_keyword_list[i])]\n",
      "    new_key.append(a)\n",
      "    \n",
      "print(new_key[0])\n",
      "com_keyword_list = new_key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['cd', 'interaction', 'complex', 'emittance', 'benign', 'cdmof', 'ph', 'cdmofs', 'curcumin', 'cyclodextrinmetal']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(man_keyword_list[0])\n",
      "print(com_keyword_list[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['cdmofs', 'curcumin', 'encapsulation', 'spectroscopy', 'stability', 'cyclodextrin']\n",
        "['cd', 'interaction', 'complex', 'emittance', 'benign', 'cdmof', 'ph', 'cdmofs', 'curcumin', 'cyclodextrinmetal']\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_TP_TN(man,com):\n",
      "    TP, FP, FN = 0,0,0\n",
      "    for i in range(len(man)):\n",
      "        man_set = set(man[i])\n",
      "        com_set = set(com[i])\n",
      "        pTP = len(man_set&com_set)\n",
      "        pFP = len(com_set-man_set)\n",
      "        pFN = len(man_set-com_set)\n",
      "        TP += pTP\n",
      "        FP += pFP\n",
      "        FN += pFN\n",
      "    return TP,FP,FN\n",
      "\n",
      "TP_FP_FN = get_TP_TN(man_keyword_list, com_keyword_list)\n",
      "print(TP_FP_FN)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1865, 7080, 5949)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_precision(word_set):\n",
      "    precision = word_set[0]/(word_set[0]+word_set[1])\n",
      "    return precision\n",
      "\n",
      "def get_recall(word_set):\n",
      "    recall = word_set[0]/(word_set[0]+word_set[2])\n",
      "    return recall\n",
      "\n",
      "def get_f_score(word_set):\n",
      "    p = get_precision(word_set)\n",
      "    r = get_recall(word_set)\n",
      "    f_score = 2*p*r/(p+r)\n",
      "    return f_score\n",
      "\n",
      "print ('Precision=',get_precision(TP_FP_FN))\n",
      "print ('Recall=',get_recall(TP_FP_FN))\n",
      "print ('F-score=',get_f_score(TP_FP_FN))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Precision= 0.20849636668529906\n",
        "Recall= 0.23867417455848478\n",
        "F-score= 0.22256697893669072\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}